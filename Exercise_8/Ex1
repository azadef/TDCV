close all; clear all; clc;

%sample points to plot error rate with the iterations. We take 30 points to
%plot the graph
sample_points = [1:9 ceil(logspace(1, 3, 21))];

% for 3 data file we have n=3
for n = 1:3
    filename = sprintf('data%d.mat', n);
    outputname = sprintf('output_%d.png', n);
    load(filename);
    
    %taking the X and Y values for data file. labels is the groundtruth
    %result
    samples = dat(:, 1:2);
    labels = dat(:, end);
    
    %ploting the groundtruth
    h = figure();
    subplot(2, 1, 1)
    hold on
    index = labels == 1;
    plot(samples(index, 1), samples(index, 2), 'r*');
    plot(samples(~index, 1), samples(~index, 2), 'b*');
    title(sprintf('Groundtruth on dataset #%d', n))

    %plotting the classifier result
    subplot(2, 1, 2)
    hold on
    clf = AdaboostClassifier(1000);
    clf.train(samples, labels);
    classification = clf.test(samples);
    index = classification == 1;
    plot(samples(index, 1), samples(index, 2), 'r*');
    plot(samples(~index, 1), samples(~index, 2), 'b*');
    title(sprintf('Classification on dataset #%d', n))
    print(h, '-dpng', outputname);
    
    %calculating error
    errors = zeros(size(sample_points));
    for m = 1:numel(sample_points)
        clf = AdaboostClassifier(sample_points(m));
        clf.train(samples, labels);
        classification = clf.test(samples);
        errors(m) = mean(classification ~= labels);
    end
    
    %plotting for error
    h = figure();
    plot(sample_points, errors, 'g-', 'LineWidth', 2);
    title(sprintf('Error on dataset #%d', n))
    print(h, '-dpng', sprintf('error_%d.png', n));    
end